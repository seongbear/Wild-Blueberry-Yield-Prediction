# -*- coding: utf-8 -*-
"""Wild blueberry Yield Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r81g-9x0LbvqObgRmvJoNyoc7FCoch8y

# 0.Load Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import warnings
import joblib
# %pip install pandas-profiling
import ydata_profiling

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

from statsmodels.formula import api
from sklearn.feature_selection import RFE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.decomposition import PCA
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.metrics import mean_absolute_error

from sklearn.linear_model import PoissonRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, roc_auc_score

"""# 1.Load Dataset"""

# Read the csv file with specified column names
df = pd.read_csv('WildBlueberryPollinationSimulationData.csv')

# Display the first 5 rows of the dataset
df.head()

# get last 5 row of the dataset
df.tail()

"""# 2.Exploratory Data Analysis (EDA) and Data Preprocessing
-Analyze the given dataset. Explore the dataset to gain insights into the data. Identify the features and the target variable in the dataset.

## 2.1 Basic Info of Datasets

13 features with 1 target variable (yield)

* Clonesize (m2) - The average blueberry clone size in the field
* Honeybee bees (/m2/min) - Honeybee density in the field
* Bumbles bees (/m2/min) - Bumblebee density in the field
* Andrena bees (/m2/min) - Andrena bee density in the field
* Osmia bees (/m2/min) - Osmia bee density in the field
* MaxOfUpperTRange (℃) - The highest record of the upper band daily air temperature during the bloom season
* MinOfUpperTRange (℃) - The lowest record of the upper band daily air temperature
* AverageOfUpperTRange (℃) - The average of the upper band daily air temperature
* MaxOfLowerTRange (℃) - The highest record of the lower band daily air temperature
* MinOfLowerTRange (℃) - The lowest record of the lower band daily air temperature
* AverageOfLowerTRange (℃) - The average of the lower band daily air temperature
* RainingDays (Day) - The total number of days during the bloom season, each of which has precipitation larger than zero
* AverageRainingDays (Day) - The average of raining days of the entire bloom season
* Yield

Before diving into details analysis, try performing a quick and brief EDA on the dataset to have glimpse onto the features' characteristics.
"""

#Performing brief EDA onto the dataset
report = ydata_profiling.ProfileReport(df, minimal=False)## Quick EDA
report.to_notebook_iframe()

"""From the result of quick analysis, we can glimpse some useful information and also noticed some potential issues in the dataset."""

# Get statistics on the dataset and round to 2 decimal places
df.describe().round(2)

# Check the count and data type for each column and can check for NaN/missing values
df.info()

df.columns

"""## 2.2 Handling Missing Values and Duplicates

Firstly, we deal with the missing values.

To prevent biased or misleading results, we handle the missing values to avoid inaccurate or misleading results, as missing vlaues may skew the distribution, affect summary statistics, or impact the relationships between variables.
"""

# Check missing values
df.isnull().sum()

"""No missing values are found in the dataset"""

duplicates = df.duplicated()
print("Number of duplicate records:", duplicates.sum())
duplicate_rows = df[df.duplicated(keep=False)]
duplicate_rows

"""No duplicates is found in the dataset.

## 2.3 Visualization
"""

# Histogram
# Visualize the distribution of numerical features and label
df.hist(figsize=(20,20))

"""The histogram above is used to provide a quick visual summary of the distribution of each feature, including the range of values, the frequency of different values, and shape of the distribution. This can help to identify any patterns or trends in the data and to better understand the characteristics of the dataset."""

df = pd.DataFrame(df)

# Create box plots for each column against the target "yield"
columns_to_plot = df.columns.difference(["yield", "fruitmass","fruitset", "seeds"])  # Exclude 'yield', 'fruitmass', 'fruitset', 'seeds' column
for column in columns_to_plot:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=column, y='yield', data=df)
    plt.title(f'Boxplot of {column} vs. Yield')
    plt.ylabel('Yield')
    plt.xlabel(column)
    plt.show()

# Create scatter plots for each column against the target "yield"
columns_to_plot = ["fruitmass", "fruitset", "seeds"]  # Include 'yield', 'fruitmass', 'fruitset', 'seeds' columns only
for column in columns_to_plot:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=column, y='yield', data=df)
    plt.title(f'Scatterplot of {column} vs. Yield')
    plt.ylabel('Yield')
    plt.xlabel(column)
    plt.show()

"""## 2.4 Outlier Analysis

### Boxplot visualization
"""

# Boxplot
numeric_cols = {"clonesize","honeybee","bumbles","andrena","osmia","MaxOfUpperTRange","MinOfUpperTRange","AverageOfUpperTRange","MaxOfLowerTRange","MinOfLowerTRange","AverageOfLowerTRange","RainingDays","AverageRainingDays","fruitset","fruitmass","seeds","yield"}
# create boxplot for each numerical columns
fig, axes = plt.subplots(nrows=1, ncols=len(numeric_cols), figsize=(30, 15))

# i for index, col for column name
for i, col in enumerate(numeric_cols):
    sns.boxplot(df[col], ax=axes[i])
    axes[i].set_title(col)

plt.tight_layout()
plt.show()

"""We observed that there are some outliers for a few columns. Besides, we will consider their truthfulness before handling them. (E.g., we will be calculating their skewness of data, deciding whether to deal with the outliers or just leave it after consideration)

### Skewness for Numerical Features
"""

# Check the skewness for each columns
for col in numeric_cols:
    skewness = df[col].skew()
    print("Skewness of {}: {}".format(col, skewness))

# # print("Skewness of price: " + str(df['price'].skew()))
# # print("Skewness of area: " + str(df['area'].skew()))

"""**Note**: The data is considered normal if the skewness is between -2 to 2 (Hair et al., 2010).

If the skewness value of the numerical features is still at this range, then we neither remove or impute the outliers to prevent loss of information, which can affect the accuracy of the model.
else we have to replace them by lower or upper limit.

**Thus, following the inferences, for 'honeybee' feature, we are going to remove the outliers.**
"""

# Calculate IQR and outlier limits
q1 = df['honeybee'].quantile(0.25)
q3 = df['honeybee'].quantile(0.75)
iqr = q3 - q1
lower = q1 - (iqr * 1.5)
upper = q3 + (iqr * 1.5)

# Remove outliers from 'honeybee' column
df = df[(df['honeybee'] >= lower) & (df['honeybee'] <= upper)]

"""### Boxplot visualization after outlier removal"""

# Boxplot
numeric_cols = {"clonesize","honeybee","bumbles","andrena","osmia","MaxOfUpperTRange","MinOfUpperTRange","AverageOfUpperTRange","MaxOfLowerTRange","MinOfLowerTRange","AverageOfLowerTRange","RainingDays","AverageRainingDays","fruitset","fruitmass","seeds","yield"}
# create boxplot for each numerical columns
fig, axes = plt.subplots(nrows=1, ncols=len(numeric_cols), figsize=(30, 10))

# i for index, col for column name
for i, col in enumerate(numeric_cols):
    sns.boxplot(df[col], ax=axes[i])
    axes[i].set_title(col)

plt.tight_layout()
plt.show()

"""## 2.5 Correlation Matrix

Correlation matrix:

* To find the correlation between the features and the target variable
* Check the correlation coefficients to see which variables are highly correlated
"""

fig, ax = plt.subplots(figsize=(20, 10))
mask = np.triu(np.ones_like(df.corr(), dtype=bool))  # Create a mask for the upper triangular portion
sns.heatmap(df.corr(), vmin=-1, vmax=1, cmap='BrBG', annot=True, mask=mask, ax=ax)
plt.title("Correlation Heatmap")
plt.show()

"""### Correlation Analysis

1. Top 3 features that are highly correlated with the label 'yield'

*   fruitset (0.99)
*   seeds (0.97)
*   fruitmass (0.94)

2. There are several features in the dataset that exhibit high correlation with each other, indicating the presence of multicollinearity.

## 2.6 Remove Unnecessary Columns
"""

# List of columns to remove
columns_to_remove = ['AverageRainingDays', 'MaxOfUpperTRange','MinOfUpperTRange','MaxOfLowerTRange','MinOfLowerTRange','fruitset','seeds','fruitmass']

# Remove the specified columns
df = df.drop(columns=columns_to_remove, axis=1)

df

"""## 2.7 PCA"""

pca = PCA(random_state=37)
pca.fit(X_train)
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)

plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')
plt.xticks(np.arange(0, X_train.shape[1] + 1, 1))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Variance Explained by PCA Components')
plt.grid(True)

plt.figure(figsize=(6, 4))
plt.xticks(range(X_train.shape[1]), range(1, X_train.shape[1] + 1, 1))
plt.bar(range(X_train.shape[1]), pca.explained_variance_ratio_, alpha=0.5, align='center',
        label='individual explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')
plt.tight_layout()

pca = PCA(n_components=8, random_state=37)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

# Convert to dataframe
component_names = [f"PC{i+1}" for i in range(X_train.shape[1])]
X_train = pd.DataFrame(X_train, columns=component_names)

X_train.head()

"""## 2.8 Split the dataset into training and testing sets"""

# Separate features (X) and target (y)
X = df.drop(columns=["yield"])  # Features
y = df["yield"]  # Target

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train: " + str(X_train.shape))
print("y_train: " + str(y_train.shape))

print("X_test: " + str(X_test.shape))
print("y_test: " + str(y_test.shape))

"""# 3.Model Building

## 3.1 Poisson Regression
"""

# Create and fit a Poisson Regression model
poisson_reg = PoissonRegressor()
poisson_reg.fit(X_train, y_train)

# Make predictions using the Poisson Regression model
y_pred = poisson_reg.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the calculated metrics
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared (R2) Score:", r2)

"""## 3.2 Linear Regression"""

# Create and fit a Linear Regression model
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

# Make predictions using the Linear Regression model
y_pred = linear_reg.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the calculated metrics
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared (R2) Score:", r2)

"""## 3.3 Ridge Regression"""

# Create and fit a Ridge Regression model
ridge_reg = Ridge()
ridge_reg.fit(X_train, y_train)

# Make predictions using the Ridge Regression model
y_pred = ridge_reg.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the calculated metrics
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared (R2) Score:", r2)

"""## 3.4 Polynomial Regression"""

# Create polynomial features
poly = PolynomialFeatures(degree=2)  # Set the degree as needed
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Create and fit a Linear Regression model with polynomial features
poly_reg = LinearRegression()
poly_reg.fit(X_train_poly, y_train)

# Make predictions using the Polynomial Regression model
y_pred = poly_reg.predict(X_test_poly)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the calculated metrics
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared (R2) Score:", r2)

"""## 3.5 Lasso Regression"""

# Create and fit a Lasso Regression model
lasso_reg = Lasso(alpha=0.01)  # Set the alpha parameter as needed
lasso_reg.fit(X_train, y_train)

# Make predictions using the Lasso Regression model
y_pred = lasso_reg.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the calculated metrics
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared (R2) Score:", r2)

"""## 3.6 Elastic Net Regression"""

# Create and fit an Elastic Net Regression model
elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)
elastic_net.fit(X_train, y_train)

# Make predictions using the Elastic Net Regression model
y_pred = elastic_net.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the calculated metrics
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared (R2) Score:", r2)

"""# 4.Model Evaluation

## Hyperparameter Tuning of Polynomial Regression
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline  # Import the Pipeline class

# Load your dataset and split it into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define your dataset (replace this with your actual data)
X = pd.DataFrame(["clonesize","honeybee","bumbles","andrena","osmia","AverageOfUpperTRange","AverageOfLowerTRange","RainingDays"])  # Your features
y = pd.Series("yield")     # Your target variable

# Create a pipeline with PolynomialFeatures and LinearRegression
pipeline = Pipeline([
    ('poly', PolynomialFeatures(include_bias=False)),
    ('regressor', LinearRegression())
])

# Define the hyperparameters and their possible values
param_grid = {
    'poly__degree': [2, 3, 4],  # Test different polynomial degrees
    'regressor__fit_intercept': [True, False]  # Test different intercept options
}

# Create a GridSearchCV object
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the GridSearchCV object to your data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and corresponding score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Negative Mean Squared Error:", grid_search.best_score_)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions using the best model
y_pred = best_model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the calculated metrics
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared (R2) Score:", r2)

"""# 5.Save Model"""

joblib.dump(best_model, 'best_model.pkl')

# Load the saved model
etc_model = joblib.load('best_model.pkl')

for i in range(3):
    X_row = X_test.iloc[i].to_frame().T
    print("Example " + str(i+1) + ": \n" + str(X_row))
    y_pred = etc_model.predict(X_row)
    print("Prediction: " + str(y_pred))

# Method 1:
input_data = {'clonesize': [25.0],
              'honeybee': [0.75],
              'bumbles': [0.25],
              'andrena': [0.25],
              'osmia': [0.25],
              'AverageOfUpperTRange': [71.9],
              'AverageOfLowerTRange': [50.8],
              'RainingDays': [16]}
input_df = pd.DataFrame(input_data)
print("Example: \n" + str(input_df))
y_pred = etc_model.predict(input_df)
print("Prediction: " + str(y_pred))